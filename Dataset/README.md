# CROSS-MODAL IMAGE-MUSIC DATASET

+ Our cross-modal dataset consists of two existing image datasets, IAPS[1] and NAPS[2], as well as a proposed music dataset, each labeled with VA  values. We calculate emotional distances between images and music by computing the Euclidean distance between their VA labels. 
+ For copyright reasons, we cannot provide the IAPS[1] and NAPS[2] image datasets in this repository. Please obtain from their respective official sources. Our repository only includes the  music dataset proposed in this paper and its corresponding label information.



[1] P. J. Lang, M. M. Bradley, and B. N. Cuthbert, “International Affective Picture System (IAPS): Technical Manual and Affective ratings,” *NIMH Center for the Study of Emotion and Attention*, vol. 1, no. 39–58, p. 3, 1997.

[2] A. Marchewka, Lukasz Żurawski, K. Jednoróg, and A. Grabowska, “The Nencki Affective Picture System (NAPS): Introduction to a Novel, Standardized, Wide-Range, High-Quality, Realistic Picture Database,” *Behavior research methods*, vol. 46, pp. 596–610, 2014.